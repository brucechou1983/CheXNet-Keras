[DEFAULT]
; working directory, one working directory can only have one running job at a time
output_dir=./experiments/1

; all images should be placed under this dir
image_source_dir=./data/images/

; number of train/dev unique patient count,
; test patient count will be (total count - train_patient_count - dev_patient_count), which is 389 by default
train_patient_count=28744
dev_patient_count=1672

; download this file directly from NIH dropbox
data_entry_file=./data/Data_Entry_2017.csv

; class names, you should not modify this
class_names=Atelectasis,Cardiomegaly,Effusion,Infiltration,Mass,Nodule,Pneumonia,Pneumothorax,Consolidation,Edema,Emphysema,Fibrosis,Pleural_Thickening,Hernia

[TRAIN]
; file path of imagenet pretrained weights, loaded at base_model initialization step
use_base_model_weights=true
base_model_weights_file=./densenet121_weights_tf.h5

; if true, load trained model weights saved in output_dir
; this is typically used for resuming your previous training tasks
; so the use_split_dataset will be automatically set to false
; also, make sure you use the reasonable initial_learning_rate
use_trained_model_weights=false
; if true, use best weights, else use last weights
use_best_weights=false

; note that the best weighting will be saved as best_weights.h5
output_weights_name=weights.h5

; basic training parameters
epochs=20
batch_size=16
initial_learning_rate=0.001

; steps per epoch for training
; auto or int
; if auto is set, (total samples / batch_size / 10) is used by default.
train_steps=auto

; steps per epoch for validation
; auto or int
; if auto is set, (total samples / batch_size / 5) is used by default.
validation_steps=auto

; patience parameter used for ReduceLROnPlateau callback
; If val_loss doesn't decrease for x epochs, learning rate will be reduced by factor of 10.
patience_reduce_lr=1

; this variable controlls the class_weight ratio between 0 and 1
; higher value means higher weighting of positive samples
positive_weights_multiply=1

; if true, mean binary cross-entroy will be weighted by positive counts
; if false, just average the cross-entropy loss of each class
; in both cases, positive/negative sample ratio of each class is considered in the cross-entropy loss
use_class_balancing=false

; if true, use default split, otherwise create a new train/dev/test split
use_default_split=true

; if "use_default_split" is false and this is false, randomly create a new train/dev/test split
use_skip_split=true

; random state used for splitting dataset into train/dev/test sets
split_dataset_random_state=1

; print model summary
show_model_summary=true

[TEST]
batch_size=32
test_generator_random_state=1
; if true, use best_weights.h5, else use weights.h5
use_best_weights=true
